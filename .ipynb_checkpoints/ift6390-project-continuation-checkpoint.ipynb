{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#Process the data through tf and tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer_noidf = TfidfTransformer(use_idf=False)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parentpath='/home/orlandom/Documents/UdeM/A2017/IFT6390/Project/'\n",
    "\n",
    "#Load files in the IMDB train set\n",
    "mypath=parentpath + 'aclImdb/train/pos/'\n",
    "files_pos_train = [mypath + f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath=parentpath + 'aclImdb/train/neg/'\n",
    "files_neg_train = [mypath + f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "#Load files in the IMDB test set\n",
    "mypath=parentpath + 'aclImdb/test/pos/'\n",
    "files_pos_test = [mypath + f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "mypath=parentpath + 'aclImdb/test/neg/'\n",
    "files_neg_test = [mypath + f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "\n",
    "files = files_pos_train + files_pos_test + files_neg_train + files_neg_test\n",
    "\n",
    "vocab_file = open(parentpath + 'aclImdb/imdb.vocab', \"r\")\n",
    "vocab = vocab_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer with supplied vocabulary and using a bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigram\n",
    "cv_wo_stop_w = CountVectorizer(input='filename', stop_words='english', \n",
    "                                   min_df=3, max_df=0.95, dtype='int32', vocabulary=vocab)\n",
    "corpus_wo_stop_w = cv_wo_stop_w.fit_transform(files)\n",
    "\n",
    "cv_w_stop_w = CountVectorizer(input='filename', min_df=3, max_df=0.95, dtype='int32', vocabulary=vocab)\n",
    "corpus_w_stop_w = cv_w_stop_w.fit_transform(files)\n",
    "\n",
    "#Bigram\n",
    "cv_wo_stop_w_bi = CountVectorizer(input='filename', min_df=3, max_df=0.95, dtype='int32', ngram_range=(1, 2),\n",
    "                                  stop_words='english') #vocabulary=vocab,\n",
    "corpus_wo_stop_w_bi = cv_wo_stop_w_bi.fit_transform(files)\n",
    "\n",
    "cv_w_stop_w_bi = CountVectorizer(input='filename', min_df=3, max_df=0.95, dtype='int32', \n",
    "                                 ngram_range=(1, 2)) #vocabulary=vocab,\n",
    "corpus_w_stop_w_bi = cv_w_stop_w_bi.fit_transform(files)\n",
    "\n",
    "#Trigram\n",
    "cv_wo_stop_w_tri = CountVectorizer(input='filename', min_df=3, max_df=0.95, dtype='int32', ngram_range=(1, 3),\n",
    "                                  stop_words='english') #vocabulary=vocab,\n",
    "corpus_wo_stop_w_tri = cv_wo_stop_w_tri.fit_transform(files)\n",
    "\n",
    "cv_w_stop_w_tri = CountVectorizer(input='filename', min_df=3, max_df=0.95, dtype='int32', \n",
    "                                 ngram_range=(1, 3)) #vocabulary=vocab,\n",
    "corpus_w_stop_w_tri = cv_w_stop_w_tri.fit_transform(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing stop words - trigram\n",
      "Vocabulary size: 366483\n",
      "Minimum size of doc is 3.0 and maximum size is 1040.0\n",
      "Mean size is 142.20958 and variance is 8696.30005622\n",
      "Leaving stop words - trigram\n",
      "Vocabulary size: 905884\n",
      "Minimum size of doc is 8.0 and maximum size is 3026.0\n",
      "Mean size is 386.43874 and variance is 61867.7205272\n",
      "Removing stop words - bigram\n",
      "Vocabulary size: 308910\n",
      "Minimum size of doc is 3.0 and maximum size is 1001.0\n",
      "Mean size is 133.6765 and variance is 7720.09192775\n",
      "Leaving stop words - bigram\n",
      "Vocabulary size: 455551\n",
      "Minimum size of doc is 8.0 and maximum size is 2151.0\n",
      "Mean size is 298.88416 and variance is 36446.5194211\n",
      "Removing stop words - unigram\n",
      "Vocabulary size: 89527\n",
      "Minimum size of doc is 3.0 and maximum size is 771.0\n",
      "Mean size is 86.78736 and variance is 3545.91026423\n",
      "Leaving stop words - unigram\n",
      "Vocabulary size: 89527\n",
      "Minimum size of doc is 4.0 and maximum size is 940.0\n",
      "Mean size is 134.62792 and variance is 6129.57431647\n"
     ]
    }
   ],
   "source": [
    "def analyze_corpus_data(corpus, count_vectorizer):\n",
    "    size_vocab = len(count_vectorizer.vocabulary_)\n",
    "    size_docs = np.zeros(corpus.shape[0])\n",
    "\n",
    "    for i in range(corpus.indptr.shape[0]-1):\n",
    "        size_docs[i] = corpus.indptr[i+1] - corpus.indptr[i]\n",
    "        \n",
    "    print \"Vocabulary size:\", size_vocab\n",
    "    print \"Minimum size of doc is {} and maximum size is {}\".format(min(size_docs), max(size_docs))\n",
    "    print \"Mean size is {} and variance is {}\".format(size_docs.mean(), size_docs.var())\n",
    "\n",
    "print \"Removing stop words - trigram\"\n",
    "analyze_corpus_data(corpus_wo_stop_w_tri, cv_wo_stop_w_tri)\n",
    "print \"Leaving stop words - trigram\"\n",
    "analyze_corpus_data(corpus_w_stop_w_tri, cv_w_stop_w_tri)\n",
    "print \"Removing stop words - bigram\"\n",
    "analyze_corpus_data(corpus_wo_stop_w_bi, cv_wo_stop_w_bi)\n",
    "print \"Leaving stop words - bigram\"\n",
    "analyze_corpus_data(corpus_w_stop_w_bi, cv_w_stop_w_bi)\n",
    "print \"Removing stop words - unigram\"\n",
    "analyze_corpus_data(corpus_wo_stop_w, cv_wo_stop_w)\n",
    "print \"Leaving stop words - unigram\"\n",
    "analyze_corpus_data(corpus_w_stop_w, cv_w_stop_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(corpus, num_train=40000, num_valid=5000, num_test=5000):\n",
    "\n",
    "    #Create train, valid and test data\n",
    "    doc_index = np.arange(corpus.shape[0])\n",
    "    target_pos = np.ones(corpus.shape[0]/2).astype(int) #first 25000 docs have target 1=positive\n",
    "    target_neg = np.zeros(corpus.shape[0]/2).astype(int) #next 25000 docs have target 0=negative\n",
    "    target = np.concatenate((target_pos, target_neg), axis=0) #now we have data and target values\n",
    "\n",
    "    #shuffle all of the data and target\n",
    "    doc_index, corpus, target = shuffle(doc_index, corpus, target, random_state=0)\n",
    "\n",
    "    train_data = corpus[:num_train]\n",
    "    train_targets = target[:num_train]\n",
    "    valid_data = corpus[num_train:num_train+num_valid]\n",
    "    valid_targets = target[num_train:num_train+num_valid]\n",
    "    test_data = corpus[num_train+num_valid:]\n",
    "    test_targets = target[num_train+num_valid:]\n",
    "\n",
    "    return train_data, train_targets, valid_data, valid_targets, test_data, test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clf(clf, data):\n",
    "    \n",
    "    train_data = data[0]\n",
    "    train_targets = data[1]\n",
    "    valid_data = data[2]\n",
    "    valid_targets = data[3]\n",
    "    \n",
    "    time0 = time.time()\n",
    "\n",
    "    clf.fit(train_data, train_targets) \n",
    "    predictions_train = clf.predict(train_data)\n",
    "    error_train = 1 - (predictions_train == train_targets).mean()\n",
    "    predictions_valid = clf.predict(valid_data)\n",
    "    error_valid = 1 - (predictions_valid == valid_targets).mean()\n",
    "\n",
    "    time1 = time.time()\n",
    "\n",
    "    print \"It took {} seconds\".format(time1-time0)\n",
    "    print \"Training error: \", error_train*100\n",
    "    print \"Validation error: \", error_valid*100\n",
    "    \n",
    "    return error_train, error_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus=[corpus_w_stop_w_tri, corpus_wo_stop_w_tri, corpus_w_stop_w_bi, \n",
    "             corpus_wo_stop_w_bi, corpus_w_stop_w, corpus_wo_stop_w]\n",
    "\n",
    "#No transformation\n",
    "data_raw = []\n",
    "data_tf = []\n",
    "data_tfidf = []\n",
    "data_scaled = []\n",
    "\n",
    "for corpus in data_corpus:\n",
    "    data_raw.append(split_data(corpus))\n",
    "    data_tf.append(split_data(tfidf_transformer_noidf.fit_transform(corpus)))\n",
    "    data_tfidf.append(split_data(tfidf_transformer.fit_transform(corpus)))\n",
    "    data_scaled.append(split_data(max_abs_scaler.fit_transform(corpus)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: Run data set with stop words-------------------\n",
      "It took 35.9928219318 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  8.66\n",
      "Trigram: Run data set without stop words-------------------\n",
      "It took 11.5295898914 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  10.16\n",
      "Bigram: Run data set with stop words-------------------\n",
      "It took 23.1913349628 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  8.94\n",
      "Bigram: Run data set with stop words-------------------\n",
      "It took 11.8183670044 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  9.98\n",
      "Unigram: Run data set with stop words-------------------\n",
      "It took 8.22762989998 seconds\n",
      "Training error:  0.005\n",
      "Validation error:  12.84\n",
      "Unigram: Run data set without stop words-------------------\n",
      "It took 5.61096692085 seconds\n",
      "Training error:  0.01\n",
      "Validation error:  12.86\n"
     ]
    }
   ],
   "source": [
    "linearSVM = svm.LinearSVC()\n",
    "\n",
    "headers = [\"Trigram: Run data set with stop words-------------------\",\n",
    "           \"Trigram: Run data set without stop words-------------------\",\n",
    "           \"Bigram: Run data set with stop words-------------------\",\n",
    "           \"Bigram: Run data set with stop words-------------------\",\n",
    "           \"Unigram: Run data set with stop words-------------------\",\n",
    "           \"Unigram: Run data set without stop words-------------------\"]\n",
    "\n",
    "for (header, corpus) in zip(headers, data_raw):\n",
    "    print header\n",
    "    eval_clf(linearSVM, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run data after doing only TF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Trigram: Run data set with stop words-------------------\n",
      "It took 3.37059187889 seconds\n",
      "Training error:  0.825\n",
      "Validation error:  8.28\n",
      "TF Trigram: Run data set without stop words-------------------\n",
      "It took 0.945204973221 seconds\n",
      "Training error:  0.925\n",
      "Validation error:  9.0\n",
      "TF Bigram: Run data set with stop words-------------------\n",
      "It took 1.9431810379 seconds\n",
      "Training error:  1.3825\n",
      "Validation error:  8.38\n",
      "TF Bigram: Run data set with stop words-------------------\n",
      "It took 0.866637945175 seconds\n",
      "Training error:  1.08\n",
      "Validation error:  8.9\n",
      "TF Unigram: Run data set with stop words-------------------\n",
      "It took 0.841536998749 seconds\n",
      "Training error:  6.4325\n",
      "Validation error:  9.92\n",
      "TF Unigram: Run data set without stop words-------------------\n",
      "It took 0.569452762604 seconds\n",
      "Training error:  3.8625\n",
      "Validation error:  9.8\n"
     ]
    }
   ],
   "source": [
    "headers = [\"TF Trigram: Run data set with stop words-------------------\",\n",
    "           \"TF Trigram: Run data set without stop words-------------------\",\n",
    "           \"TF Bigram: Run data set with stop words-------------------\",\n",
    "           \"TF Bigram: Run data set with stop words-------------------\",\n",
    "           \"TF Unigram: Run data set with stop words-------------------\",\n",
    "           \"TF Unigram: Run data set without stop words-------------------\"]\n",
    "\n",
    "for (header, corpus) in zip(headers, data_tf):\n",
    "    print header\n",
    "    eval_clf(linearSVM, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run previous data after doing TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-IDF Trigram: Run data set with stop words-------------------\n",
      "It took 3.103525877 seconds\n",
      "Training error:  0.01\n",
      "Validation error:  7.64\n",
      "TF-IDF Trigram: Run data set without stop words-------------------\n",
      "It took 0.957690954208 seconds\n",
      "Training error:  0.045\n",
      "Validation error:  8.62\n",
      "TF-IDF Bigram: Run data set with stop words-------------------\n"
     ]
    }
   ],
   "source": [
    "headers = [\"TF-IDF Trigram: Run data set with stop words-------------------\",\n",
    "           \"TF-IDF Trigram: Run data set without stop words-------------------\",\n",
    "           \"TF-IDF Bigram: Run data set with stop words-------------------\",\n",
    "           \"TF-IDF Bigram: Run data set with stop words-------------------\",\n",
    "           \"TF-IDF Unigram: Run data set with stop words-------------------\",\n",
    "           \"TF-IDF Unigram: Run data set without stop words-------------------\"]\n",
    "\n",
    "for (header, corpus) in zip(headers, data_tfidf):\n",
    "    print header\n",
    "    eval_clf(linearSVM, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Trigram: Run data set with stop words-------------------\n",
      "It took 3.2905421257 seconds\n",
      "Training error:  0.01\n",
      "Validation error:  7.64\n",
      "TF-IDF Trigram: Run data set without stop words-------------------\n",
      "It took 1.02786397934 seconds\n",
      "Training error:  0.045\n",
      "Validation error:  8.62\n",
      "TF-IDF Bigram: Run data set with stop words-------------------\n",
      "It took 1.83157110214 seconds\n",
      "Training error:  0.04\n",
      "Validation error:  7.8\n",
      "TF-IDF Bigram: Run data set without stop words-------------------\n",
      "It took 0.92861199379 seconds\n",
      "Training error:  0.0675\n",
      "Validation error:  8.74\n",
      "TF-IDF Unigram: Run data set with stop words-------------------\n",
      "It took 0.615745782852 seconds\n",
      "Training error:  1.5775\n",
      "Validation error:  9.68\n",
      "TF-IDF Unigram: Run data set without stop words-------------------\n",
      "It took 0.495291948318 seconds\n",
      "Training error:  1.36\n",
      "Validation error:  9.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.013599999999999945, 0.098999999999999977)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print  \"TF-IDF Trigram: Run data set with stop words-------------------\"\n",
    "eval_clf(linearSVM, data_tfidf_corpus_w_stop_w_tri)\n",
    "\n",
    "print  \"TF-IDF Trigram: Run data set without stop words-------------------\"\n",
    "eval_clf(linearSVM, data_tfidf_corpus_wo_stop_w_tri)\n",
    "\n",
    "print  \"TF-IDF Bigram: Run data set with stop words-------------------\"\n",
    "eval_clf(linearSVM, data_tfidf_corpus_w_stop_w_bi)\n",
    "\n",
    "print  \"TF-IDF Bigram: Run data set without stop words-------------------\"\n",
    "eval_clf(linearSVM, data_tfidf_corpus_wo_stop_w_bi)\n",
    "\n",
    "print  \"TF-IDF Unigram: Run data set with stop words-------------------\"\n",
    "eval_clf(linearSVM, data_tfidf_corpus_w_stop_w)\n",
    "\n",
    "print  \"TF-IDF Unigram: Run data set without stop words-------------------\"\n",
    "eval_clf(linearSVM, data_tfidf_corpus_wo_stop_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run previous data after scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "data_scaled_corpus_w_stop_w_tri = split_data(max_abs_scaler.fit_transform(corpus_w_stop_w_tri))\n",
    "data_scaled_corpus_wo_stop_w_tri = split_data(max_abs_scaler.fit_transform(corpus_wo_stop_w_tri))\n",
    "data_scaled_corpus_w_stop_w_bi = split_data(max_abs_scaler.fit_transform(corpus_w_stop_w_bi))\n",
    "data_scaled_corpus_wo_stop_w_bi = split_data(max_abs_scaler.fit_transform(corpus_wo_stop_w_bi))\n",
    "data_scaled_corpus_w_stop_w = split_data(max_abs_scaler.fit_transform(corpus_w_stop_w))\n",
    "data_scaled_corpus_wo_stop_w = split_data(max_abs_scaler.fit_transform(corpus_wo_stop_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Trigram: Run data set with stop words-------------------\n",
      "It took 53.3931951523 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  8.48\n",
      "Scale Trigram: Run data set without stop words-------------------\n",
      "It took 19.4017632008 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  10.26\n",
      "Scale Bigram: Run data set with stop words-------------------\n",
      "It took 31.1804440022 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  8.76\n",
      "Scale Bigram: Run data set without stop words-------------------\n",
      "It took 13.9711098671 seconds\n",
      "Training error:  0.0\n",
      "Validation error:  10.52\n",
      "Scale Unigram: Run data set with stop words-------------------\n",
      "It took 2.75061607361 seconds\n",
      "Training error:  0.1075\n",
      "Validation error:  11.78\n",
      "Scale Unigram: Run data set without stop words-------------------\n",
      "It took 2.41466188431 seconds\n",
      "Training error:  0.12\n",
      "Validation error:  12.32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0011999999999999789, 0.12319999999999998)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print  \"Scale Trigram: Run data set with stop words-------------------\"\n",
    "eval_clf(linearSVM, data_scaled_corpus_w_stop_w_tri)\n",
    "\n",
    "print  \"Scale Trigram: Run data set without stop words-------------------\"\n",
    "eval_clf(linearSVM, data_scaled_corpus_wo_stop_w_tri)\n",
    "\n",
    "print  \"Scale Bigram: Run data set with stop words-------------------\"\n",
    "eval_clf(linearSVM, data_scaled_corpus_w_stop_w_bi)\n",
    "\n",
    "print  \"Scale Bigram: Run data set without stop words-------------------\"\n",
    "eval_clf(linearSVM, data_scaled_corpus_wo_stop_w_bi)\n",
    "\n",
    "print  \"Scale Unigram: Run data set with stop words-------------------\"\n",
    "eval_clf(linearSVM, data_scaled_corpus_w_stop_w)\n",
    "\n",
    "print  \"Scale Unigram: Run data set without stop words-------------------\"\n",
    "eval_clf(linearSVM, data_scaled_corpus_wo_stop_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.262296915054 seconds\n",
      "Training error:  7.1875\n",
      "Validation error:  10.98\n",
      "Scale Trigram: Run data set with stop words-------------------\n",
      "It took 1.34645104408 seconds\n",
      "Training error:  0.025\n",
      "Validation error:  8.38\n",
      "It took 0.459006786346 seconds\n",
      "Training error:  0.78\n",
      "Validation error:  9.92\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0078000000000000291, 0.099199999999999955)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=15, tol=None)\n",
    "\n",
    "eval_clf(sgd_clf, data_scaled_corpus_wo_stop_w)\n",
    "\n",
    "print  \"Scale Trigram: Run data set with stop words-------------------\"\n",
    "eval_clf(sgd_clf, data_scaled_corpus_w_stop_w_tri)\n",
    "\n",
    "\n",
    "eval_clf(sgd_clf, data_scaled_corpus_wo_stop_w_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_svm_rbf(corpus, num_train=40000, num_valid=5000, num_test=5000):\n",
    "    doc_index = np.arange(corpus.shape[0])\n",
    "    target_pos = np.ones(corpus.shape[0]/2).astype(int) #first 25000 docs have target 1=positive\n",
    "    target_neg = np.zeros(corpus.shape[0]/2).astype(int) #next 25000 docs have target 0=negative\n",
    "    target = np.concatenate((target_pos, target_neg), axis=0) #now we have data and target values\n",
    "\n",
    "    #shuffle all of the data and target\n",
    "    doc_index, corpus, target = shuffle(doc_index, corpus, target, random_state=0)\n",
    "\n",
    "    train_data = corpus[:num_train]\n",
    "    train_targets = target[:num_train]\n",
    "    valid_data = corpus[num_train:num_train+num_valid]\n",
    "    valid_targets = target[num_train:num_train+num_valid]\n",
    "    test_data = corpus[num_train+num_valid:]\n",
    "    test_targets = target[num_train+num_valid:]\n",
    "    \n",
    "    time0 = time.time()\n",
    "\n",
    "    clf_rbf = svm.SVC()\n",
    "    clf_rbf.fit(train_data, train_targets) \n",
    "    predictions_train = clf_rbf.predict(train_data)\n",
    "    error_train_rbf = 1 - (predictions_train == train_targets).mean()\n",
    "    predictions_valid = clf_rbf.predict(valid_data)\n",
    "    error_valid_rbf = 1 - (predictions_valid == valid_targets).mean()\n",
    "\n",
    "    time1 = time.time()\n",
    "\n",
    "    print \"It took {} seconds\".format(time1-time0)\n",
    "    print \"Training error: \", error_train_rbf\n",
    "    print \"Validation error: \", error_valid_rbf\n",
    "    \n",
    "    return error_train_rbf, error_valid_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Run with RBF kernel-------------------------------------------------------------\"\n",
    "print  \"Preprocessing after TF-IDF Trigram: Run data set with stop words-------------------\"\n",
    "eval_svm_rbf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w_tri)))\n",
    "\n",
    "print  \"Preprocessing after TF-IDF Trigram: Run data set without stop words-------------------\"\n",
    "eval_svm_rbf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_wo_stop_w_tri)))\n",
    "\n",
    "print  \"Preprocessing after TF-IDF Bigram: Run data set with stop words-------------------\"\n",
    "eval_svm_rbf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w_bi)))\n",
    "\n",
    "print  \"Preprocessing after TF-IDF Bigram: Run data set without stop words-------------------\"\n",
    "eval_svm_rbf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_wo_stop_w_bi)))\n",
    "\n",
    "print  \"Preprocessing after TF-IDF Unigram: Run data set with stop words-------------------\"\n",
    "eval_svm_rbf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w)))\n",
    "\n",
    "print  \"Preprocessing after TF-IDF Unigram: Run data set without stop words-------------------\"\n",
    "eval_svm_rbf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_wo_stop_w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=1e-5, random_state=1)\n",
    "\n",
    "# print  \"TF-IDF Trigram: Run data set with stop words-------------------\"\n",
    "# # eval_svm_linear(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), with_mean=False))\n",
    "# eval_clf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w_tri)))\n",
    "\n",
    "# print  \"TF-IDF Trigram: Run data set without stop words-------------------\"\n",
    "# eval_SGD(tfidf_transformer.fit_transform(corpus_wo_stop_w_tri))\n",
    "\n",
    "# print  \"TF-IDF Bigram: Run data set with stop words-------------------\"\n",
    "# eval_SGD(tfidf_transformer.fit_transform(corpus_w_stop_w_bi))\n",
    "\n",
    "# print  \"TF-IDF Bigram: Run data set without stop words-------------------\"\n",
    "# eval_SGD(tfidf_transformer.fit_transform(corpus_wo_stop_w_bi))\n",
    "\n",
    "# print  \"TF-IDF Unigram: Run data set with stop words-------------------\"\n",
    "# eval_SGD(tfidf_transformer.fit_transform(corpus_w_stop_w))\n",
    "\n",
    "# print  \"TF-IDF Unigram: Run data set without stop words-------------------\"\n",
    "# eval_SGD(tfidf_transformer.fit_transform(corpus_wo_stop_w))\n",
    "eval_clf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_wo_stop_w)), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_clf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w)), clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MLP on trigram bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=1e-5, random_state=1)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=1e-3, random_state=1)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(10,10,10), alpha=1e-5, random_state=1,\n",
    "                    learning_rate='adaptive', verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=1e-3, random_state=1)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=1e-3, verbose=True)\n",
    "eval_clf(preprocessing.maxabs_scale(tfidf_transformer.fit_transform(corpus_w_stop_w_tri)), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=1e-3, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does preprocessing help in this case? No - see two runs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=0.01, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=0.1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=0.5, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=2, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=4, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MLP on unigram bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_wo_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=0.1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10), alpha=0.01, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(20,20,20), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(20,20), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(50,50), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(30,30,30), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,10,10, 10), alpha=1, verbose=True)\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5,5,5), alpha=0.5, activation='logistic')\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_wo_stop_w), clf)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_wo_stop_w_tri), clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w), clf)\n",
    "\n",
    "clf = GaussianNB()\n",
    "eval_clf(tfidf_transformer.fit_transform(corpus_wo_stop_w), clf)\n",
    "\n",
    "# clf = MultinomialNB()\n",
    "# eval_clf(tfidf_transformer.fit_transform(corpus_w_stop_w_tri), clf)\n",
    "\n",
    "# clf = MultinomialNB()\n",
    "# eval_clf(tfidf_transformer.fit_transform(corpus_wo_stop_w_tri), clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
